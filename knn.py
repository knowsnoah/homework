# -*- coding: utf-8 -*-
"""knn

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z20DeTAKrp6MJ0LJu8_1QaSG_cJWPfz9
"""

#-------------------------------------------------------------------------
# AUTHOR: Noah Ojeda
# FILENAME: knn.py
# # SPECIFICATION: Computes 1-NN Leave-One-Out Cross-Validation error rate
#   for spam/ham classification using the file 'email_classification.csv'.
# FOR: CS 4210 - Assignment #2
# TIME SPENT: ~30 minutes
#-----------------------------------------------------------*/
#IMPORTANT NOTE: YOU ARE ALLOWED TO USE ANY PYTHON LIBRARY TO COMPLETE THIS PROGRAM
#Importing some Python libraries
from sklearn.neighbors import KNeighborsClassifier
import pandas as pd

#Reading the data in a csv file using pandas
db = []
df = pd.read_csv('email_classification.csv')
for _, row in df.iterrows():
  db.append(row.tolist())

errors = 0
total = len(db)

#Loop your data to allow each instance to be your test set
for i in range(total):
  #Add the training features to the 20D array X removing the instance that will be used for testing in this iteration.
  #For instance, X = [[1, 2, 3, 4, 5, ..., 20]].
  #Convert each feature value to float to avoid warning messages
  #--> add your Python code here
  X = []
  for j in range(total):
    if j != i:
      features = [] #adding the first 20 features as floats
      for k in range(20):
        features.append(float(db[j][k]))
      X.append(features)

  #Transform the original training classes to numbers and add them to the vector Y.
  #Do not forget to remove the instance that will be used for testing in this iteration.
  #For instance, Y = [1, 2, ,...].
  #Convert each feature value to float to avoid warning messages
  #--> add your Python code here
  Y = []
  for j in range(total):
    if j != i:
      Y.append(db[j][20])

  #Store the test sample of this iteration in the vector testSample
  #--> add your Python code here
  testSample = []
  for k in range(20):
    testSample.append(float(db[i][k]))
  trueLabel = db[i][20]

  #Fitting the knn to the data using k = 1 and Euclidean distance (L2 norm)
  #--> add your Python code here
  clf = KNeighborsClassifier(n_neighbors=1, metric='euclidean')
  clf.fit(X, Y)

  #Use your test sample in this iteration to make the class prediction. For instance:
  #class_predicted = clf.predict([[1, 2, 3, 4, 5, ..., 20]])[0]
  #--> add your Python code here
  class_predicted = clf.predict([testSample])[0]

  #Compare the prediction with the true label of the test instance to start calculating the error rate.
  #--> add your Python code here
  if class_predicted != trueLabel:
    errors +=1

#Print the error rate
#--> add your Python code here
error_rate = errors / total
print(f" The LOO-CV error rate for 1NN is{error_rate: 0.5f}")

